{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88a4744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.25821231 0.30172295 0.23200198 0.29394135 0.21472745 0.32247892\n",
      " 0.2849275  0.20660196 0.22767541 0.27185435 0.18857985 0.18129039\n",
      " 0.19257389 0.33281773 0.34008468 0.25019763 0.22274705 0.25178482\n",
      " 0.24413654 0.26029173 0.33902724 0.29094962 0.28267677 0.22850129\n",
      " 0.23284266 0.22946836 0.23489306 0.28680289 0.32755954 0.23569595\n",
      " 0.31756162 0.21015094 0.29145329 0.2789894  0.29074686 0.20574142\n",
      " 0.18003506 0.19918101 0.35782841 0.26085658 0.19307014 0.28677103\n",
      " 0.20727517 0.21902598 0.20245988 0.25986811 0.21292731 0.2905099\n",
      " 0.33710651 0.18783341 0.25711817 0.1487615  0.23424912 0.32314208\n",
      " 0.18832677 0.24289654 0.25341691 0.23948789 0.28848627 0.28789422\n",
      " 0.27865951 0.23201972 0.27019054 0.13135476 0.15460793 0.25859746\n",
      " 0.35336115 0.24119133 0.28740981 0.25826318 0.25174257 0.36044485\n",
      " 0.29174526 0.25755114 0.1900859  0.23193826 0.28825481 0.24293227\n",
      " 0.28120394 0.30597476 0.30340595 0.21712643 0.25794815 0.31237413\n",
      " 0.26151397 0.26288547 0.28484754 0.29991491 0.22416667 0.22567387\n",
      " 0.34177702 0.26626249 0.27156431 0.23910708 0.29031009 0.31359484\n",
      " 0.23342606 0.32958231 0.29333813 0.23354906 0.15638151 0.33141741\n",
      " 0.29240842 0.23960397 0.2467906  0.23167517 0.14490247 0.18562861\n",
      " 0.16826944 0.16909885 0.3661246  0.2932982  0.29987759 0.15443585\n",
      " 0.24457919 0.2103484  0.21960753 0.28492182 0.09575202 0.25620595\n",
      " 0.19457518 0.281134   0.30933771 0.25531357 0.20226447 0.19884492\n",
      " 0.23868144 0.27282099 0.28111146 0.29218483 0.23901993 0.2409266\n",
      " 0.18791561 0.29763835 0.25240986 0.27124763 0.29118735 0.12800748\n",
      " 0.19789561 0.25882635 0.29311202 0.32070509 0.2951368  0.23940557\n",
      " 0.2201544  0.34182251 0.23891318 0.28760763 0.3095033  0.21363018\n",
      " 0.3095028  0.36536038 0.23281126 0.32177028 0.26472514 0.24954018\n",
      " 0.27635223 0.25569329 0.33712912 0.32277929 0.32679522 0.28913965\n",
      " 0.21595624 0.31872655 0.1674457  0.26568197 0.34690359 0.2841217\n",
      " 0.30062945 0.19998414 0.23148388 0.24939591 0.25357341 0.31966574\n",
      " 0.21465426 0.1763238  0.23167967 0.26388878 0.24245538 0.1842927\n",
      " 0.27771116 0.23950075 0.26820198 0.19865014 0.2507057  0.23694412\n",
      " 0.2774062  0.26059221 0.26513573 0.36218395 0.29636342 0.30516062\n",
      " 0.34059695 0.25109559 0.23126453 0.25764984 0.21752684 0.1980223\n",
      " 0.25342937 0.20777748 0.3473983  0.25528012 0.32307091 0.23244083\n",
      " 0.24809844 0.21265092 0.18422137 0.18237903 0.26031423 0.25098835\n",
      " 0.20112506 0.21921114 0.30839945 0.34992633 0.28718524 0.1885892\n",
      " 0.2055209  0.21707194 0.21760029 0.2128522  0.26992478 0.2430556\n",
      " 0.20228433 0.31071197 0.37579908 0.29753917 0.26585498 0.21365861\n",
      " 0.2520642  0.29155416 0.21751855 0.26238015 0.21783466 0.2371435\n",
      " 0.2449543  0.20183418 0.30169702 0.27614795 0.24694043 0.32589203\n",
      " 0.32476433 0.28541009 0.22149885 0.17070794 0.3033169  0.23483692\n",
      " 0.1531669  0.22609903 0.29457102 0.24705026 0.22561479 0.25203744\n",
      " 0.54131621 0.53766911 0.47552165 0.41886046 0.46220236 0.54835763\n",
      " 0.45034534 0.51196221 0.47212749 0.39579705 0.40988089 0.426544\n",
      " 0.41934639 0.54290535 0.44743263 0.47183662 0.5440242  0.56937547\n",
      " 0.46633657 0.48973409 0.56245133 0.38743019 0.54176332 0.52219449\n",
      " 0.44430919 0.50963369 0.43409326 0.52115792 0.46037078 0.55779641\n",
      " 0.47283473 0.4975602  0.45583424 0.42513593 0.52887361 0.52646008\n",
      " 0.52915579 0.42684901 0.60971895 0.4911216  0.41364734 0.41338043\n",
      " 0.51397092 0.49062086 0.50967391 0.45914147 0.53899384 0.46506646\n",
      " 0.41539836 0.50337428 0.49131689 0.38867439 0.54464407 0.48473973\n",
      " 0.4644115  0.48030934 0.48209261 0.51473417 0.53106002 0.49816734\n",
      " 0.54571439 0.61659629 0.47803735 0.54684852 0.5285747  0.56545949\n",
      " 0.53781501 0.47396129 0.43085487 0.44172142 0.49973588 0.51178611\n",
      " 0.51451647 0.48318807 0.51803458 0.49904402 0.53526865 0.56927679\n",
      " 0.45019918 0.51574673 0.5046423  0.48087065 0.49412745 0.59510328\n",
      " 0.46969408 0.53640497 0.46248862 0.48215409 0.51577418 0.58566993\n",
      " 0.49139129 0.50863336 0.48641675 0.50822613 0.54962702 0.5164179\n",
      " 0.59209228 0.57423826 0.59389012 0.4451354  0.47556705 0.49308778\n",
      " 0.53931006 0.53249276 0.52774543 0.48823877 0.49723526 0.57585965\n",
      " 0.46960681 0.41908981 0.42116018 0.47219223 0.46637158 0.53016928\n",
      " 0.48578134 0.47582679 0.47990688 0.5130942  0.48095011 0.46763988\n",
      " 0.57711633 0.50335418 0.56540044 0.54314881 0.52884889 0.52308119\n",
      " 0.39407668 0.38004229 0.43579881 0.52139722 0.47857492 0.46586597\n",
      " 0.52278784 0.39771673 0.41152729 0.49982216 0.46315277 0.54663022\n",
      " 0.53717941 0.46119173 0.4156301  0.52308625 0.56518358 0.61323805\n",
      " 0.52458016 0.55164455 0.48389988 0.46390559 0.53284779 0.50935721\n",
      " 0.61957332 0.51263042 0.48537064 0.51817192 0.41896488 0.43344833\n",
      " 0.44238413 0.45821262 0.48403664 0.497332   0.523814   0.45898243\n",
      " 0.50605294 0.54162842 0.47271665 0.54849055 0.39253915 0.49734729\n",
      " 0.54808088 0.55475715 0.55560287 0.51613514 0.43125776 0.49622711\n",
      " 0.42400987 0.40867062 0.48534577 0.53735772 0.60661296 0.64806074\n",
      " 0.45924641 0.54174421 0.5045212  0.5268342  0.50395467 0.56263345\n",
      " 0.4758451  0.6045266  0.54484637 0.50788799 0.45073679 0.54877441\n",
      " 0.40924095 0.57387083 0.51558668 0.56950997 0.49018838 0.5609757\n",
      " 0.57928007 0.52901307 0.4917384  0.4841204  0.44212196 0.53800549\n",
      " 0.53818196 0.55269811 0.46836983 0.53485493 0.5453506  0.44515989\n",
      " 0.52562701 0.47070827 0.46790557 0.53811427 0.46887123 0.61399618\n",
      " 0.48457896 0.57647655 0.56503717 0.50796101 0.56802455 0.49295759\n",
      " 0.46093003 0.51355058 0.45918863 0.43419508 0.39878732 0.39884243\n",
      " 0.53815931 0.50063082 0.54259605 0.48699918 0.47053866 0.53144623\n",
      " 0.51923363 0.46817316 0.45143593 0.50493407 0.41616555 0.43972171\n",
      " 0.52032144 0.49264292 0.45920716 0.43342746 0.477499   0.49995708\n",
      " 0.51109902 0.50394057 0.46162295 0.46275517 0.45912464 0.47108537\n",
      " 0.50123719 0.4720604  0.56149    0.45549719 0.59838231 0.4737573\n",
      " 0.54348478 0.53427749 0.47797845 0.44647282 0.45998024 0.53361804\n",
      " 0.50283767 0.55686805 0.65795874 0.48992723 0.49340287 0.49064296\n",
      " 0.44942478 0.48329753 0.45709255 0.53428899 0.40327397 0.53881563\n",
      " 0.44356583 0.55450215 0.47111711 0.46147296 0.33029292 0.43301137\n",
      " 0.52653608 0.50311873 0.46922585 0.49993436 0.55307791 0.51263454\n",
      " 0.51842013 0.53712875 0.50841497 0.48375728 0.51285443 0.48010104\n",
      " 0.46133278 0.52866177 0.47425125 0.51045739 0.49668954 0.46615215\n",
      " 0.50739447 0.54901617 0.50925814 0.42797903 0.51972667 0.53564886\n",
      " 0.49306022 0.50810117 0.50415592 0.52272324 0.56542804 0.58497254\n",
      " 0.50764238 0.57440641 0.51371121 0.4034028  0.44494411 0.47677355\n",
      " 0.54942278 0.53706329 0.48327157 0.49741529 0.56598974 0.44486751\n",
      " 0.48780248 0.48158339 0.48582112 0.46749512 0.49753649 0.41018762\n",
      " 0.48686669 0.51833278 0.53337011 0.51810573 0.53806689 0.52649834\n",
      " 0.51287455 0.57822788 0.49635016 0.53482979 0.50834078 0.50008723\n",
      " 0.56541004 0.49467262 0.49586886 0.53720108 0.45907064 0.420919\n",
      " 0.57767043 0.49035634 0.47404354 0.47989655 0.47622466 0.44566321\n",
      " 0.40513842 0.45115853 0.53992248 0.44370009 0.49917621 0.53690531\n",
      " 0.50864556 0.45411719 0.45113451 0.54843477 0.56114464 0.48201355\n",
      " 0.49906516 0.59556589 0.47333239 0.56527272 0.46027578 0.50897058\n",
      " 0.48787326 0.3752681  0.55328237 0.49057341 0.52325675 0.49674584\n",
      " 0.46951404 0.52092046 0.50241097 0.50080224 0.65410395 0.52009153\n",
      " 0.55015052 0.49493894 0.53145273 0.44389112 0.51474639 0.43097897\n",
      " 0.57888042 0.48453023 0.45687153 0.52589214 0.49547545 0.49613242\n",
      " 0.57857045 0.49676699 0.49541233 0.48667502 0.52202917 0.40426752\n",
      " 0.49308163 0.45266403 0.48561344 0.56309563 0.52122375 0.46274666\n",
      " 0.45552319 0.57147283 0.46825581 0.47186796 0.48465755 0.53166376\n",
      " 0.58064674 0.52314825 0.50505001 0.52551103 0.40313613 0.4807184\n",
      " 0.48980317 0.48432487 0.56133833 0.46422155 0.52625434 0.50318424\n",
      " 0.49780116 0.50905766 0.57099973 0.4459379  0.50414587 0.50705061\n",
      " 0.55356064 0.4471986  0.50447044 0.44821067 0.43848532 0.52049637\n",
      " 0.53503444 0.44745075 0.4676407  0.54862742 0.53018722 0.5259168\n",
      " 0.41660374 0.51160674 0.50361431 0.46986275 0.52193038 0.44398604\n",
      " 0.51714672 0.33655699 0.40665836 0.48356133 0.5666144  0.39272268\n",
      " 0.52418314 0.46917184 0.44882685 0.47524292 0.51429827 0.57460693\n",
      " 0.48977255 0.54106273 0.50354111 0.49511244 0.5880315  0.49059079\n",
      " 0.58736196 0.52647357 0.54706104 0.51818227 0.57207951 0.62728184\n",
      " 0.53767649 0.55616173 0.49219055 0.43272199 0.4684941  0.43076498\n",
      " 0.48837429 0.56091712 0.48066179 0.41294385 0.37212182 0.4947006\n",
      " 0.50750032 0.44069062 0.49486569 0.52956569 0.51121387 0.53350843\n",
      " 0.53915575 0.46320435 0.46622822 0.5239662  0.62941883 0.44591223\n",
      " 0.51670425 0.66389834 0.66796745 0.79559074 0.69023942 0.87290981\n",
      " 0.70753196 0.77481436 0.7336541  0.78417038 0.70442541 0.78134322\n",
      " 0.7870222  0.8215971  0.6661177  0.63396777 0.68802238 0.7630301\n",
      " 0.76968697 0.70121078 0.77363927 0.75004761 0.76019395 0.74526583\n",
      " 0.73899679 0.71152954 0.68596083 0.79193986 0.83453399 0.80273093\n",
      " 0.70477733 0.63664061 0.8089088  0.70232214 0.85935445 0.75692734\n",
      " 0.7944113  0.8032522  0.69642497 0.66685352 0.68145984 0.72983266\n",
      " 0.63624985 0.78881534 0.74410347 0.75380489 0.83487769 0.6838611\n",
      " 0.87511599 0.69763652 0.71285866 0.75450756 0.7178246  0.72120343\n",
      " 0.77884072 0.75054517 0.82582165 0.72725666 0.74438918 0.80431365\n",
      " 0.75604643 0.74258329 0.72462219 0.72721476 0.84075185 0.68237998\n",
      " 0.73146155 0.78917944 0.82283785 0.71426869 0.72698506 0.82823817\n",
      " 0.74210908 0.72130912 0.79271319 0.73638292 0.75086654 0.73454228\n",
      " 0.78042442 0.74286493 0.77142156 0.79533139 0.81061405 0.7856633\n",
      " 0.77090359 0.80789967 0.79958823 0.73939512 0.65459882 0.68742665\n",
      " 0.79892199 0.74217664 0.68050636 0.73119046 0.7409592  0.71585141\n",
      " 0.74619362 0.73110255 0.80939363 0.71916633 0.71096564 0.73767569\n",
      " 0.71800536 0.77754373 0.67889266 0.73575018 0.6855701  0.75297337\n",
      " 0.71786504 0.69048482 0.78468992 0.71040802 0.71689507 0.8206889\n",
      " 0.79748646 0.71910789 0.85845496 0.69928568 0.77232941 0.77859359\n",
      " 0.73258427 0.74020619 0.74017029 0.74579848 0.70594564 0.76379237\n",
      " 0.76154742 0.66711237 0.81081348 0.73740387 0.78876929 0.71425283\n",
      " 0.82333889 0.68746444 0.77464719 0.83394745 0.73226749 0.77422166\n",
      " 0.70320824 0.82171626 0.77039669 0.71262388 0.71102777 0.76818778\n",
      " 0.79649651 0.76149287 0.7153197  0.77288182 0.75465852 0.71716372\n",
      " 0.77777158 0.78370243 0.85180395 0.7816755  0.69666442 0.7337311\n",
      " 0.71142463 0.71792606 0.74540107 0.75196002 0.80360899 0.73749035\n",
      " 0.82929466 0.8036221  0.83723988 0.71355028 0.69007636 0.73292401\n",
      " 0.73859114 0.74155631 0.73753299 0.65569251 0.7474747  0.77186838\n",
      " 0.7113523  0.76766188 0.77496284 0.78969012 0.78330145 0.69696393\n",
      " 0.65545471 0.83300556 0.70432183 0.7680997  0.76158438 0.59043394\n",
      " 0.65770034 0.82188683 0.73316991 0.73921041 0.82884874 0.72786081\n",
      " 0.80563581 0.75759069 0.79724897 0.76750532 0.72955879 0.71034086\n",
      " 0.70229525 0.77167668 0.68410309 0.69327357 0.68073519 0.79071484\n",
      " 0.76919074 0.77716851 0.76526007 0.7453571  0.70430465 0.76818182\n",
      " 0.74741061 0.80655603 0.63420268 0.68757509 0.76132239 0.69263726\n",
      " 0.67316535 0.81619107 0.74477348 0.77433441 0.68852053 0.70951037\n",
      " 0.74041905 0.8434546  0.7804131  0.79060716 0.77002272 0.75919835\n",
      " 0.76224354 0.72022772 0.74854798 0.70975754 0.82309944 0.67299803\n",
      " 0.6828657  0.75938364 0.72892972 0.65497795 0.77417603 0.70895473\n",
      " 0.73789959 0.74386974 0.74308649 0.77718237 0.82712519 0.70447515\n",
      " 0.74896174 0.76848879 0.68570582]\n",
      "Restart 0, Iteration 0: Log Likelihood -16676.5508\n",
      "Restart 0, Iteration 1: Log Likelihood 807.0260\n",
      "Restart 0, Iteration 2: Log Likelihood 869.2092\n",
      "Best likelihood: 869.209228515625\n",
      "Fitted GMM parameters:\n",
      "Means: [[0.4993492 0.4993492 0.4993492]]\n",
      "Std devs: [[0.18105309 0.18105309 0.18105309]]\n",
      "Weights: [[1. 1. 1.]]\n",
      "1\n",
      "[0.21117132 0.3055923  0.31742965 ... 0.68999228 0.77694684 0.75962862]\n",
      "Restart 0, Iteration 0: Log Likelihood -20934.5527\n",
      "Restart 0, Iteration 1: Log Likelihood 860.0125\n",
      "Restart 0, Iteration 2: Log Likelihood 888.5884\n",
      "Best likelihood: 888.5884399414062\n",
      "Fitted GMM parameters:\n",
      "Means: [[0.5037888 0.5037888 0.5037888]]\n",
      "Std devs: [[0.18190125 0.18190125 0.18190125]]\n",
      "Weights: [[1. 1. 1.]]\n",
      "2\n",
      "[0.19008689 0.28982945 0.1540658  0.18304795 0.16631331 0.29391854\n",
      " 0.26303147 0.22650813 0.26758008 0.24363804 0.24611173 0.21247626\n",
      " 0.32210067 0.13533389 0.14976515 0.33788594 0.25145266 0.21566998\n",
      " 0.29173526 0.22728979 0.22897097 0.36203274 0.26776253 0.23301923\n",
      " 0.30243909 0.2450744  0.32415883 0.25002611 0.23913526 0.31372165\n",
      " 0.24366005 0.1707522  0.16420874 0.29962011 0.29050258 0.26499485\n",
      " 0.29075542 0.23376886 0.18415879 0.20961161 0.29363322 0.21299718\n",
      " 0.21141995 0.23830748 0.2091007  0.22270264 0.26877877 0.26299296\n",
      " 0.23132959 0.20268405 0.26665657 0.24650938 0.24382728 0.27640392\n",
      " 0.34683851 0.22703912 0.21802409 0.38504832 0.25898352 0.20021365\n",
      " 0.26725496 0.35166878 0.20503233 0.21654868 0.29411857 0.28984311\n",
      " 0.27011215 0.29813456 0.20982069 0.21678267 0.32433009 0.30488899\n",
      " 0.26225148 0.17753936 0.21838815 0.21394389 0.25804795 0.26033067\n",
      " 0.2803498  0.16718856 0.23543554 0.26864648 0.3590668  0.29075815\n",
      " 0.16887099 0.31485541 0.22007415 0.09537556 0.29438908 0.25180783\n",
      " 0.22239436 0.24816308 0.27862421 0.25359992 0.15950972 0.22959495\n",
      " 0.26700813 0.26929311 0.27873335 0.30369152 0.24866803 0.21061084\n",
      " 0.25087301 0.15012496 0.23067536 0.21642431 0.24190748 0.32040262\n",
      " 0.2934502  0.24017659 0.22931667 0.32099037 0.25731191 0.26539614\n",
      " 0.20677145 0.30449784 0.29415662 0.2475293  0.22310405 0.24758923\n",
      " 0.2170549  0.36900507 0.2191015  0.29778859 0.25675208 0.21658319\n",
      " 0.23239838 0.27210829 0.28657725 0.22419077 0.23268021 0.19393354\n",
      " 0.29243736 0.21010049 0.19832915 0.22074891 0.27251796 0.26689706\n",
      " 0.27650439 0.25533462 0.30584391 0.24923182 0.25649001 0.26208783\n",
      " 0.22831133 0.21062316 0.24119228 0.29358658 0.28634081 0.15049161\n",
      " 0.23106087 0.26564849 0.18636584 0.34214882 0.38434271 0.22716329\n",
      " 0.36416185 0.2343142  0.35660882 0.2379958  0.28242385 0.22233172\n",
      " 0.23331956 0.20509041 0.21185532 0.22271246 0.24040745 0.3490324\n",
      " 0.24792827 0.32610579 0.29238151 0.25063875 0.30459325 0.31449806\n",
      " 0.26836588 0.14224733 0.32378889 0.26888473 0.23656395 0.1997371\n",
      " 0.24331788 0.28839643 0.290844   0.26022739 0.25633421 0.26489747\n",
      " 0.34404934 0.21784727 0.19002423 0.18955304 0.18680037 0.22224634\n",
      " 0.22168982 0.26195973 0.24474842 0.2763795  0.23818655 0.21282561\n",
      " 0.19690817 0.27015551 0.1831544  0.16523572 0.32871747 0.30717763\n",
      " 0.19268187 0.2501169  0.22764622 0.22600101 0.26015938 0.22361534\n",
      " 0.19986333 0.26865512 0.27723553 0.14777112 0.22978018 0.27492838\n",
      " 0.32473202 0.25572064 0.21233221 0.2664464  0.25363626 0.29159285\n",
      " 0.28001385 0.21018704 0.30385796 0.23866337 0.21950654 0.25085013\n",
      " 0.2096507  0.21216841 0.27555854 0.27031974 0.31179816 0.29383394\n",
      " 0.23285358 0.3444183  0.25102369 0.41698623 0.52291313 0.54660564\n",
      " 0.43997438 0.50130289 0.47999037 0.51528288 0.48499556 0.52543448\n",
      " 0.43084844 0.54073183 0.54675327 0.50365995 0.52083288 0.48164524\n",
      " 0.57994434 0.55787424 0.47280655 0.45497277 0.52621946 0.47657601\n",
      " 0.52962431 0.55690148 0.59558811 0.52006547 0.56075421 0.46025091\n",
      " 0.51595731 0.55134837 0.53337597 0.53243059 0.40053637 0.51647472\n",
      " 0.483012   0.53473591 0.57916978 0.55495058 0.51486139 0.66618203\n",
      " 0.51724358 0.56873605 0.53023266 0.55242545 0.49063092 0.5389102\n",
      " 0.60588478 0.53148562 0.51637107 0.46917041 0.46301961 0.5147132\n",
      " 0.48806321 0.55426635 0.54324126 0.48620135 0.55238739 0.49621223\n",
      " 0.43554247 0.59642202 0.44937937 0.44615739 0.46333367 0.5276395\n",
      " 0.56314938 0.56787526 0.45699385 0.52642425 0.51418801 0.44996211\n",
      " 0.54199058 0.6155439  0.47567767 0.41294193 0.3854906  0.52895369\n",
      " 0.50919699 0.51529435 0.50039624 0.55979298 0.39923484 0.43662347\n",
      " 0.47089129 0.43812628 0.50032707 0.4937191  0.53459114 0.62867579\n",
      " 0.49916627 0.47477827 0.51728648 0.55017099 0.47699327 0.49872902\n",
      " 0.48339507 0.53371695 0.48928439 0.52788886 0.39921198 0.44492206\n",
      " 0.53455326 0.47498706 0.49306879 0.39885056 0.56824261 0.48481763\n",
      " 0.57242223 0.45265765 0.60079174 0.55868004 0.41595758 0.53480567\n",
      " 0.48961965 0.47807659 0.55604067 0.58582986 0.53908276 0.4955617\n",
      " 0.51611772 0.4929636  0.5519424  0.5058558  0.50937222 0.48138222\n",
      " 0.54692856 0.47298505 0.57517094 0.53566917 0.55346233 0.51185112\n",
      " 0.51625785 0.52221486 0.55070563 0.47017459 0.47538826 0.50384196\n",
      " 0.45861459 0.51711081 0.51529355 0.45289556 0.50987459 0.50317601\n",
      " 0.58074413 0.50399436 0.44867194 0.50730776 0.48324942 0.54378842\n",
      " 0.53975342 0.48831905 0.43208155 0.54142452 0.57333005 0.52669363\n",
      " 0.46318839 0.61062971 0.43359853 0.47988442 0.44337554 0.45424774\n",
      " 0.42578161 0.52215398 0.3916971  0.44471928 0.51543417 0.51006046\n",
      " 0.43083099 0.54970202 0.42591389 0.58003636 0.51479998 0.52453899\n",
      " 0.57772575 0.5406487  0.50713074 0.53006572 0.6079441  0.44873451\n",
      " 0.56519077 0.45118142 0.44723538 0.48584888 0.41944628 0.49578769\n",
      " 0.44704365 0.55915936 0.48912308 0.48442406 0.51037763 0.49396388\n",
      " 0.46021861 0.59045049 0.41196213 0.51329053 0.44239878 0.53654965\n",
      " 0.50528092 0.48705291 0.54806829 0.50408788 0.49105881 0.47834934\n",
      " 0.50639608 0.50204361 0.51394929 0.50819477 0.47670119 0.4842337\n",
      " 0.46596345 0.57946169 0.49869527 0.49816198 0.48300282 0.41711767\n",
      " 0.47720084 0.47089734 0.51708377 0.51956884 0.53497149 0.43670022\n",
      " 0.48152945 0.47721284 0.54651175 0.4684955  0.45617875 0.44219654\n",
      " 0.49968079 0.62291682 0.57682779 0.48311478 0.54513854 0.53189458\n",
      " 0.4937431  0.47019668 0.51273294 0.40292173 0.46896027 0.59946755\n",
      " 0.55229019 0.54507155 0.51646941 0.42284394 0.49789344 0.48610845\n",
      " 0.52716754 0.50824735 0.44782948 0.5443909  0.58538367 0.45137102\n",
      " 0.50867518 0.54556296 0.5033414  0.53236848 0.44715296 0.51108133\n",
      " 0.4975202  0.51101155 0.58904346 0.52827406 0.50865527 0.52448286\n",
      " 0.54554643 0.56705965 0.57112092 0.48096779 0.55166841 0.51519204\n",
      " 0.45143687 0.47677722 0.57145521 0.51063983 0.52636868 0.57445508\n",
      " 0.38749175 0.50525049 0.48775242 0.53182628 0.50046187 0.49646155\n",
      " 0.49018723 0.53449066 0.46580141 0.56001355 0.5561452  0.54023624\n",
      " 0.43974517 0.44719328 0.59188695 0.53231553 0.41812299 0.53700035\n",
      " 0.54692394 0.46257863 0.51781606 0.51183184 0.47077864 0.387459\n",
      " 0.54104877 0.56630655 0.52322221 0.54346137 0.40289364 0.44919284\n",
      " 0.55423373 0.55570066 0.52735276 0.54338218 0.4836849  0.46586194\n",
      " 0.47650726 0.40589729 0.44322331 0.62111116 0.55751943 0.491949\n",
      " 0.49211926 0.53237817 0.51249542 0.52138899 0.47385902 0.48374697\n",
      " 0.41960819 0.50469701 0.57399947 0.42820036 0.49191359 0.41918603\n",
      " 0.56680621 0.52012966 0.44357087 0.52044703 0.43214348 0.4763406\n",
      " 0.55976048 0.42543522 0.55350145 0.44294668 0.56598874 0.54144861\n",
      " 0.47528338 0.5634553  0.49263934 0.51384551 0.57280837 0.5370718\n",
      " 0.53910453 0.47130285 0.45279476 0.544675   0.45639853 0.4369997\n",
      " 0.51477303 0.52073315 0.4010542  0.50941967 0.5376468  0.51689183\n",
      " 0.52225748 0.56316299 0.56721607 0.50676621 0.50393307 0.52343903\n",
      " 0.63726826 0.58748862 0.45192596 0.52852268 0.62994892 0.46431774\n",
      " 0.51217827 0.51669285 0.43089108 0.47185496 0.49391502 0.59777662\n",
      " 0.44675623 0.56087291 0.45395971 0.54634851 0.42496328 0.52235254\n",
      " 0.46475191 0.56526781 0.45682531 0.52428147 0.50572077 0.50639485\n",
      " 0.40967776 0.49689956 0.50600972 0.55091482 0.5470368  0.47621813\n",
      " 0.54367199 0.44084116 0.41883428 0.5344604  0.48526465 0.43937982\n",
      " 0.50099442 0.52291653 0.58405456 0.55760096 0.40427307 0.49838733\n",
      " 0.49595959 0.51653956 0.44521184 0.52675544 0.53395403 0.55785839\n",
      " 0.45173309 0.44098415 0.5572181  0.52077609 0.51977343 0.47308098\n",
      " 0.53399839 0.52248971 0.51200087 0.48002826 0.52777946 0.41168202\n",
      " 0.52686202 0.51622866 0.52002995 0.46493127 0.61024239 0.38579814\n",
      " 0.47106161 0.51945293 0.521128   0.51759998 0.43070773 0.46548433\n",
      " 0.50361062 0.42907112 0.50609801 0.47231407 0.58318729 0.54366505\n",
      " 0.50271986 0.54577157 0.58707985 0.53712821 0.49104938 0.49287141\n",
      " 0.55028269 0.46905371 0.47806139 0.55739585 0.5093586  0.50388643\n",
      " 0.49117006 0.43703143 0.49833292 0.49848901 0.50630498 0.63089276\n",
      " 0.46041937 0.46973003 0.7558484  0.62088088 0.76258418 0.68247115\n",
      " 0.72528577 0.78051189 0.70042379 0.75945043 0.79172944 0.82499946\n",
      " 0.7886962  0.76617026 0.67108813 0.80686199 0.78809347 0.78872069\n",
      " 0.80491739 0.72734855 0.71526559 0.80320467 0.71988406 0.7576912\n",
      " 0.78993959 0.78618445 0.72114831 0.73933107 0.82449805 0.66944062\n",
      " 0.75353095 0.7471678  0.77285456 0.80076761 0.70694994 0.80067415\n",
      " 0.64279671 0.71631823 0.69403315 0.62490679 0.74157768 0.77018943\n",
      " 0.73281102 0.84778996 0.77002866 0.72427884 0.66531874 0.81300256\n",
      " 0.69516799 0.76992857 0.72560778 0.74969636 0.67992591 0.74067092\n",
      " 0.75834935 0.76660124 0.72269893 0.74641784 0.72277855 0.72370815\n",
      " 0.7559734  0.72352495 0.71876649 0.73078223 0.75960589 0.71858059\n",
      " 0.88196575 0.79373576 0.73545146 0.65324641 0.75515268 0.74042929\n",
      " 0.71786633 0.73129322 0.80987762 0.73769312 0.77546951 0.75702834\n",
      " 0.72509972 0.76144161 0.78279159 0.70564528 0.68371365 0.74532454\n",
      " 0.7160241  0.80214581 0.71283275 0.7074018  0.75069989 0.80667915\n",
      " 0.86353551 0.76271541 0.7862985  0.70642865 0.75052008 0.70012264\n",
      " 0.79757992 0.69647292 0.7719718  0.73413446 0.85207823 0.69222515\n",
      " 0.68947139 0.75786327 0.69466318 0.728765   0.71697182 0.7657555\n",
      " 0.73723249 0.80313392 0.74356091 0.69241705 0.67687643 0.77712486\n",
      " 0.81878945 0.86457326 0.70446003 0.77434971 0.70287322 0.81753393\n",
      " 0.71836626 0.73536617 0.79140048 0.80635597 0.7725055  0.72039659\n",
      " 0.74569078 0.82969944 0.6962787  0.73825795 0.6884278  0.74278585\n",
      " 0.67819328 0.85153601 0.72293582 0.63317369 0.76518719 0.75890475\n",
      " 0.77492927 0.72591242 0.73180432 0.83274382 0.7107827  0.81716882\n",
      " 0.76925374 0.66215716 0.7021763  0.79339693 0.78021202 0.74006263\n",
      " 0.76182144 0.68433616 0.71123101 0.79523105 0.75397019 0.76960078\n",
      " 0.73333148 0.79392852 0.75946956 0.74820275 0.74288268 0.75143287\n",
      " 0.78372944 0.68801561 0.77448613 0.73082402 0.824245   0.78150742\n",
      " 0.7050906  0.74105375 0.66907714 0.70398252 0.71241222 0.75732066\n",
      " 0.88454157 0.75499805 0.75542391 0.72114915 0.76488857 0.79849156\n",
      " 0.7866582  0.69980719 0.71557586 0.71228347 0.79316909 0.73107615\n",
      " 0.82713702 0.81031507 0.76851652 0.70863252 0.72137615 0.7520657\n",
      " 0.78980757 0.77278736 0.76217892 0.73915095 0.71874775 0.76153668\n",
      " 0.78362026 0.77508935 0.75625307 0.70253825 0.75493074 0.78477294\n",
      " 0.77951103 0.81405684 0.78332954 0.78785817 0.71643976 0.74860996\n",
      " 0.8659912  0.61422634 0.7627655  0.80466187 0.79577206 0.72625368\n",
      " 0.72693653 0.78583992 0.76897592 0.81089046 0.6699902  0.83120533\n",
      " 0.70733779 0.81103077 0.73673835 0.66914228 0.83269591 0.63295161\n",
      " 0.81133836 0.7215652  0.75029849 0.76824691 0.77771228 0.75191339\n",
      " 0.70439935 0.74777821 0.70321606 0.77029977 0.74979757 0.71775037\n",
      " 0.64188374 0.79132709 0.67445288 0.64596876 0.70077559 0.67292044\n",
      " 0.71592064 0.78486976 0.79101438 0.68180601 0.64487003 0.73273335\n",
      " 0.74975803 0.76419408 0.80272652 0.73275636 0.72763076 0.80703928\n",
      " 0.72757987 0.76605858 0.73848963 0.77053474 0.78672952]\n",
      "Restart 0, Iteration 0: Log Likelihood -12742.7832\n",
      "Restart 0, Iteration 1: Log Likelihood 757.9411\n",
      "Restart 0, Iteration 2: Log Likelihood 761.4562\n",
      "Best likelihood: 761.4561767578125\n",
      "Fitted GMM parameters:\n",
      "Means: [[0.50875777 0.50875777 0.50875777]]\n",
      "Std devs: [[0.1857026 0.1857026 0.1857026]]\n",
      "Weights: [[1. 1. 1.]]\n",
      "3\n",
      "[0.22063831 0.17704989 0.30448971 ... 0.7075569  0.71860572 0.65141536]\n",
      "Restart 0, Iteration 0: Log Likelihood -7440.1499\n",
      "Restart 0, Iteration 1: Log Likelihood 862.3998\n",
      "Restart 0, Iteration 2: Log Likelihood 862.5868\n",
      "Best likelihood: 862.5867919921875\n",
      "Fitted GMM parameters:\n",
      "Means: [[0.5021378 0.5021378 0.5021378]]\n",
      "Std devs: [[0.18197194 0.18197194 0.18197194]]\n",
      "Weights: [[1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\"\"\"\n",
    "Gaussian Mixture Model (GMM) using PyTorch\n",
    "This code implements a simple Gaussian Mixture Model (GMM) using PyTorch.\n",
    "It includes methods for fitting the model to data and calculating the likelihood of new data points.\n",
    "\"\"\"\n",
    "\n",
    "class GaussianMixture(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super(GaussianMixture, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.means = nn.Parameter(torch.linspace(0.1, 0.9, n_components))\n",
    "        self.stds = nn.Parameter(torch.ones(n_components) * 0.1)  # Smaller initial std\n",
    "        self.weights = nn.Parameter(torch.ones(n_components) / n_components)\n",
    "        self.best_likelihood = -float('inf')\n",
    "        self.best_params = None\n",
    "        \n",
    "    def _compute_responsibilities(self, x):\n",
    "        # Expand dimensions for broadcasting\n",
    "        x = x.unsqueeze(-1)  # Shape: (n_samples, 1)\n",
    "        \n",
    "        # Calculate Gaussian probability for each component\n",
    "        gaussian_probs = torch.exp(-0.5 * ((x - self.means) / self.stds)**2) / (self.stds * torch.sqrt(torch.tensor(2 * np.pi)))\n",
    "        \n",
    "        # Weight the probabilities\n",
    "        weighted_probs = self.weights * gaussian_probs\n",
    "        \n",
    "        # Sum probabilities for all components and normalize to get responsibilities\n",
    "        total_probs = torch.sum(weighted_probs, dim=1, keepdim=True)\n",
    "        responsibilities = weighted_probs / (total_probs + 1e-10)\n",
    "        return responsibilities, torch.sum(torch.log(total_probs + 1e-10))\n",
    "\n",
    "    def _m_step(self, x, responsibilities):\n",
    "        # Update parameters based on responsibilities\n",
    "        total_resp = responsibilities.sum(0)\n",
    "        \n",
    "        # Update means\n",
    "        self.means.data = (responsibilities * x.unsqueeze(-1)).sum(0) / (total_resp + 1e-10)\n",
    "        \n",
    "        # Update standard deviations\n",
    "        variance = (responsibilities * (x.unsqueeze(-1) - self.means)**2).sum(0) / (total_resp + 1e-10)\n",
    "        self.stds.data = torch.sqrt(variance + 1e-10)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights.data = total_resp / x.shape[0]\n",
    "\n",
    "    \n",
    "    def fit(self, data, n_iterations=100, n_restarts=10, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Fit GMM using EM algorithm with multiple random restarts\n",
    "        \"\"\"\n",
    "        x = torch.from_numpy(data).float()\n",
    "        best_likelihood = -float('inf')\n",
    "        \n",
    "        for restart in range(n_restarts):\n",
    "            # Random initialization\n",
    "            with torch.no_grad():\n",
    "                self.means.data = torch.FloatTensor(self.n_components).uniform_(0.1, 0.9)\n",
    "                self.stds.data = torch.ones(self.n_components) * 0.1\n",
    "                self.weights.data = torch.ones(self.n_components) / self.n_components\n",
    "            \n",
    "            prev_likelihood = -float('inf')\n",
    "            \n",
    "            for iteration in range(n_iterations):\n",
    "                # E-step\n",
    "                responsibilities, log_likelihood = self._compute_responsibilities(x)\n",
    "                \n",
    "                # M-step\n",
    "                self._m_step(x, responsibilities)\n",
    "                \n",
    "                ## Ensure parameters stay in valid ranges\n",
    "                #with torch.no_grad():\n",
    "                #    self.means.data = torch.clamp(self.means.data, 0.0, 1.0)\n",
    "                #    self.stds.data = torch.clamp(self.stds.data, 0.001, 0.5)\n",
    "                #    self.weights.data = torch.softmax(self.weights.data, dim=0)\n",
    "                \n",
    "                # Check convergence\n",
    "                if abs(log_likelihood - prev_likelihood) < tol:\n",
    "                    break\n",
    "                    \n",
    "                prev_likelihood = log_likelihood\n",
    "                \n",
    "                # Track best parameters\n",
    "                if log_likelihood > best_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    self.best_likelihood = log_likelihood.item()\n",
    "                    self.best_params = {\n",
    "                        'means': self.means.data.clone(),\n",
    "                        'stds': self.stds.data.clone(),\n",
    "                        'weights': self.weights.data.clone()\n",
    "                    }\n",
    "                    print(f'Restart {restart}, Iteration {iteration}: Log Likelihood {log_likelihood.item():.4f}')\n",
    "        \n",
    "        # Restore best parameters\n",
    "        with torch.no_grad():\n",
    "            self.means.data = self.best_params['means']\n",
    "            self.stds.data = self.best_params['stds']\n",
    "            self.weights.data = self.best_params['weights']\n",
    "            \n",
    "        #return self.best_likelihood\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_gmm_to_ab(ind_dat, n_components):\n",
    "    \"\"\"\n",
    "    Fit Gaussian Mixture Model (GMM) to allele balance data.\n",
    "    \n",
    "    Parameters:\n",
    "        ab_dat (np.array): Allele balance data.\n",
    "        n_components (int): Number of components in the GMM.\n",
    "    \"\"\"\n",
    "    dat = ind_dat\n",
    "    #print(ind_dat.shape)\n",
    "    #print(len(ind_dat.shape))\n",
    "    # Reshape the array to 2D if necessary\n",
    "    # Example: allele_balance_array = np.random.rand(100, 10)  # Replace with actual data\n",
    "    if len(ind_dat.shape) == 1:\n",
    "        dat = ind_dat.reshape(-1, 1)\n",
    "        #print(dat.shape)\n",
    "    # Fit GMM to allele balance data   \n",
    "    gmm = GaussianMixture(n_components = n_components)\n",
    "    gmm.fit(dat)\n",
    "    # Print best likelihood\n",
    "    print(f'Best likelihood: {gmm.best_likelihood}')\n",
    "    # Print fitted parameters\n",
    "    print(\"Fitted GMM parameters:\")\n",
    "    print(f\"Means: {gmm.best_params['means'].numpy()}\")\n",
    "    print(f\"Std devs: {gmm.best_params['stds'].numpy()}\")\n",
    "    print(f\"Weights: {gmm.best_params['weights'].numpy()}\")\n",
    "    return(gmm)\n",
    "\n",
    "def plot_gmm_fit(data, gmm, n_points=1000, title=\"GMM Fit to Data\"):\n",
    "    \"\"\"\n",
    "    Plot histogram of observed data with fitted GMM components.\n",
    "    \n",
    "    Parameters:\n",
    "        data (np.array): Original data used to fit the GMM\n",
    "        gmm (GaussianMixture): Fitted GMM model\n",
    "        n_points (int): Number of points for plotting the GMM curves\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot histogram of observed data\n",
    "    plt.hist(data, bins=50, density=True, alpha=0.5, label='Observed Data')\n",
    "    \n",
    "    # Generate points for plotting the GMM components\n",
    "    x = np.linspace(min(data), max(data), n_points)\n",
    "    x_tensor = torch.from_numpy(x).float()\n",
    "    \n",
    "    # Plot individual components\n",
    "    for i in range(gmm.n_components):\n",
    "        # Calculate Gaussian distribution for this component\n",
    "        mu = gmm.means.data[i].item()\n",
    "        sigma = gmm.stds.data[i].item()\n",
    "        weight = gmm.weights.data[i].item()\n",
    "        \n",
    "        component = weight * norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, component, '--', label=f'Component {i+1}')\n",
    "    \n",
    "    # Plot total mixture\n",
    "    total = torch.zeros_like(x_tensor)\n",
    "    for i in range(gmm.n_components):\n",
    "        mu = gmm.means.data[i]\n",
    "        sigma = gmm.stds.data[i]\n",
    "        weight = gmm.weights.data[i]\n",
    "        component = weight * torch.exp(-0.5 * ((x_tensor - mu) / sigma)**2) / (sigma * torch.sqrt(torch.tensor(2 * np.pi)))\n",
    "        total += component\n",
    "    \n",
    "    plt.plot(x, total.numpy(), 'r-', label='Total Mixture', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#allele_balance_array = np.random.rand(1000, 3)\n",
    "ab_left = np.random.normal(0.25, 0.05, (500, 4))\n",
    "ab_middle = np.random.normal(0.5, 0.05, (1000, 4))\n",
    "ab_right = np.random.normal(0.75, 0.05, (500,4))\n",
    "allele_balance_array = np.concatenate([ab_left, ab_middle, ab_right], axis=0)\n",
    "#print(allele_balance_array)\n",
    "allele_mask_array = np.random.randint(0, 2, size=(2000, 4))\n",
    "ab_dat = np.array([allele_balance_array,allele_mask_array])\n",
    "#print(ab_dat)\n",
    "for i in range(len(ab_dat[0,0,:])):\n",
    "    print(i)\n",
    "    ind_dat = ab_dat[0,:,i]\n",
    "    #print(ind_dat)\n",
    "    ind_mask = (ab_dat[1,:,i] == 1)\n",
    "    #print(ind_mask)\n",
    "    ind_dat_filtered = ind_dat[ind_mask]\n",
    "    print(ind_dat_filtered)\n",
    "    gmm = fit_gmm_to_ab(ind_dat_filtered, 3)\n",
    "    \n",
    "    #plot_gmm_fit(ind_dat_filtered, gmm, title=\"Allele Balance Distribution\")\n",
    "\n",
    "#print(allele_balance_array)\n",
    "#print(allele_balance_array[:,2])\n",
    "#allele_balance_df = pd.DataFrame(allele_balance_array, columns=[f\"Sample_{i}\" for i in range(1, 11)])\n",
    "#allele_balance_df['Sample_ID'] = [f\"Sample_{i}\" for i in range(1, 101)]\n",
    "#allele_balance_df.set_index('Sample_ID', inplace=True)\n",
    "#allele_balance_df = allele_balance_df.transpose()\n",
    "#print(allele_balance_df)\n",
    "#allele_balance_df = allele_balance_df.reset_index()\n",
    "#allele_balance_df.rename(columns={'index': 'Sample_ID'}, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
